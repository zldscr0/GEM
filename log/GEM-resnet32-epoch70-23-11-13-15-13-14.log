{'data_root': '/data/bzx_yjy/cifar100', 'image_size': 32, 'pin_memory': False, 'augment': True, 'workers': 8, 'device_ids': 1, 'n_gpu': 1, 'seed': 1993, 'deterministic': True, 'epoch': 70, 'batch_size': 128, 'val_per_epoch': 5, 'optimzer': {'name': 'SGD', 'kwargs': {'lr': 0.1}}, 'lr_scheduler': {'name': 'StepLR', 'kwargs': {'gamma': 0.5, 'step_size': 30}}, 'warmup': 3, 'includes': ['headers/data.yaml', 'headers/device.yaml', 'headers/model.yaml', 'headers/optimizer.yaml', 'backbones/resnet12.yaml'], 'save_path': './', 'init_cls_num': 20, 'inc_cls_num': 20, 'task_num': 5, 'optimizer': {'name': 'SGD', 'kwargs': {'lr': 0.1, 'momentum': 0.95, 'weight_decay': 0.0002}}, 'backbone': {'name': 'resnet32', 'kwargs': None}, 'buffer': {'name': 'LinearBuffer', 'kwargs': {'buffer_size': 0, 'batch_size': 32, 'strategy': 'random'}}, 'classifier': {'name': 'GEM', 'kwargs': {'num_class': 100, 'feat_dim': 64, 'n_memories': 5120, 'n_task': 5, 'memory_strength': 0}}, 'rank': 0}
GEM(
  (net): CifarResNet(
    (conv_1_3x3): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (stage_1): Sequential(
      (0): ResNetBasicblock(
        (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): ResNetBasicblock(
        (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): ResNetBasicblock(
        (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): ResNetBasicblock(
        (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (4): ResNetBasicblock(
        (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (stage_2): Sequential(
      (0): ResNetBasicblock(
        (conv_a): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): DownsampleA(
          (avg): AvgPool2d(kernel_size=1, stride=2, padding=0)
        )
      )
      (1): ResNetBasicblock(
        (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): ResNetBasicblock(
        (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): ResNetBasicblock(
        (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (4): ResNetBasicblock(
        (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (stage_3): Sequential(
      (0): ResNetBasicblock(
        (conv_a): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): DownsampleA(
          (avg): AvgPool2d(kernel_size=1, stride=2, padding=0)
        )
      )
      (1): ResNetBasicblock(
        (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): ResNetBasicblock(
        (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): ResNetBasicblock(
        (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (4): ResNetBasicblock(
        (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
    (fc): Linear(in_features=64, out_features=10, bias=True)
  )
  (classifier): Linear(in_features=64, out_features=100, bias=True)
  (ce): CrossEntropyLoss()
)
Trainable params in the model: 470654
SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    initial_lr: 0.1
    lr: 0.03333333333333333
    maximize: False
    momentum: 0.95
    nesterov: False
    weight_decay: 0.0002
)
================Task 0 Start!================
SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    initial_lr: 0.1
    lr: 0.03333333333333333
    maximize: False
    momentum: 0.95
    nesterov: False
    weight_decay: 0.0002
)
================Task 0 Training!================
The training samples number: 10000
learning rate: [0.03333333333333333]
================ Train on the train set ================
Epoch [0/70] |	Loss: 2.744 	Average Acc: 0.172 
learning rate: [0.06666666666666667]
================ Train on the train set ================
Epoch [1/70] |	Loss: 2.269 	Average Acc: 0.300 
learning rate: [0.1]
================ Train on the train set ================
Epoch [2/70] |	Loss: 2.065 	Average Acc: 0.360 
learning rate: [0.1]
================ Train on the train set ================
Epoch [3/70] |	Loss: 1.840 	Average Acc: 0.430 
learning rate: [0.1]
================ Train on the train set ================
Epoch [4/70] |	Loss: 1.647 	Average Acc: 0.478 
================ Test on the test set ================
 * Average Acc: 0.450 Best acc 0.450
 Per-Task Acc:[0.45]
learning rate: [0.1]
================ Train on the train set ================
Epoch [5/70] |	Loss: 1.479 	Average Acc: 0.541 
learning rate: [0.1]
================ Train on the train set ================
Epoch [6/70] |	Loss: 1.299 	Average Acc: 0.591 
learning rate: [0.1]
================ Train on the train set ================
Epoch [7/70] |	Loss: 1.157 	Average Acc: 0.627 
learning rate: [0.1]
================ Train on the train set ================
Epoch [8/70] |	Loss: 1.065 	Average Acc: 0.663 
learning rate: [0.1]
================ Train on the train set ================
Epoch [9/70] |	Loss: 0.934 	Average Acc: 0.697 
================ Test on the test set ================
 * Average Acc: 0.510 Best acc 0.510
 Per-Task Acc:[0.51]
learning rate: [0.1]
================ Train on the train set ================
Epoch [10/70] |	Loss: 0.877 	Average Acc: 0.717 
learning rate: [0.1]
================ Train on the train set ================
Epoch [11/70] |	Loss: 0.779 	Average Acc: 0.745 
learning rate: [0.1]
================ Train on the train set ================
Epoch [12/70] |	Loss: 0.674 	Average Acc: 0.776 
learning rate: [0.1]
================ Train on the train set ================
Epoch [13/70] |	Loss: 0.630 	Average Acc: 0.794 
learning rate: [0.1]
================ Train on the train set ================
Epoch [14/70] |	Loss: 0.511 	Average Acc: 0.833 
================ Test on the test set ================
 * Average Acc: 0.460 Best acc 0.510
 Per-Task Acc:[0.46]
learning rate: [0.1]
================ Train on the train set ================
Epoch [15/70] |	Loss: 0.480 	Average Acc: 0.842 
learning rate: [0.1]
================ Train on the train set ================
Epoch [16/70] |	Loss: 0.463 	Average Acc: 0.845 
learning rate: [0.1]
================ Train on the train set ================
Epoch [17/70] |	Loss: 0.401 	Average Acc: 0.865 
learning rate: [0.1]
================ Train on the train set ================
Epoch [18/70] |	Loss: 0.337 	Average Acc: 0.888 
learning rate: [0.1]
================ Train on the train set ================
Epoch [19/70] |	Loss: 0.295 	Average Acc: 0.900 
================ Test on the test set ================
 * Average Acc: 0.510 Best acc 0.510
 Per-Task Acc:[0.51]
learning rate: [0.1]
================ Train on the train set ================
Epoch [20/70] |	Loss: 0.302 	Average Acc: 0.899 
learning rate: [0.1]
================ Train on the train set ================
Epoch [21/70] |	Loss: 0.254 	Average Acc: 0.917 
learning rate: [0.1]
================ Train on the train set ================
Epoch [22/70] |	Loss: 0.204 	Average Acc: 0.931 
learning rate: [0.1]
================ Train on the train set ================
Epoch [23/70] |	Loss: 0.174 	Average Acc: 0.940 
learning rate: [0.1]
================ Train on the train set ================
Epoch [24/70] |	Loss: 0.246 	Average Acc: 0.920 
================ Test on the test set ================
 * Average Acc: 0.520 Best acc 0.520
 Per-Task Acc:[0.52]
learning rate: [0.1]
================ Train on the train set ================
Epoch [25/70] |	Loss: 0.234 	Average Acc: 0.923 
learning rate: [0.1]
================ Train on the train set ================
Epoch [26/70] |	Loss: 0.169 	Average Acc: 0.944 
learning rate: [0.1]
================ Train on the train set ================
Epoch [27/70] |	Loss: 0.147 	Average Acc: 0.953 
learning rate: [0.1]
================ Train on the train set ================
Epoch [28/70] |	Loss: 0.133 	Average Acc: 0.959 
learning rate: [0.1]
================ Train on the train set ================
Epoch [29/70] |	Loss: 0.162 	Average Acc: 0.946 
================ Test on the test set ================
 * Average Acc: 0.560 Best acc 0.560
 Per-Task Acc:[0.56]
learning rate: [0.1]
================ Train on the train set ================
Epoch [30/70] |	Loss: 0.178 	Average Acc: 0.941 
learning rate: [0.1]
================ Train on the train set ================
Epoch [31/70] |	Loss: 0.180 	Average Acc: 0.939 
learning rate: [0.05]
================ Train on the train set ================
Epoch [32/70] |	Loss: 0.070 	Average Acc: 0.977 
learning rate: [0.05]
================ Train on the train set ================
Epoch [33/70] |	Loss: 0.014 	Average Acc: 0.998 
learning rate: [0.05]
================ Train on the train set ================
Epoch [34/70] |	Loss: 0.005 	Average Acc: 1.000 
================ Test on the test set ================
 * Average Acc: 0.650 Best acc 0.650
 Per-Task Acc:[0.65]
learning rate: [0.05]
================ Train on the train set ================
Epoch [35/70] |	Loss: 0.003 	Average Acc: 1.000 
learning rate: [0.05]
================ Train on the train set ================
Epoch [36/70] |	Loss: 0.002 	Average Acc: 1.000 
learning rate: [0.05]
================ Train on the train set ================
Epoch [37/70] |	Loss: 0.002 	Average Acc: 1.000 
learning rate: [0.05]
================ Train on the train set ================
Epoch [38/70] |	Loss: 0.002 	Average Acc: 1.000 
learning rate: [0.05]
================ Train on the train set ================
Epoch [39/70] |	Loss: 0.002 	Average Acc: 1.000 
================ Test on the test set ================
 * Average Acc: 0.660 Best acc 0.660
 Per-Task Acc:[0.66]
learning rate: [0.05]
================ Train on the train set ================
Epoch [40/70] |	Loss: 0.002 	Average Acc: 1.000 
learning rate: [0.05]
================ Train on the train set ================
