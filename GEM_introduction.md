## GEM-introduction

##### 范例集

按照类别增量学习过程的定义, 模型在学习第 𝑏 个任务时仅能获取当前的训练集$D^b$. 仅使用$D^b$更新模型很容易遭受灾难性遗忘. 因此, 当前类别增量学习的主流算法提出保存<u>一个额外的范例集合</u> (*exemplar/example set*), 记作$\epsilon$, 用于为每个见过的类别保存一定数目的代表性样本. 保存的样本可以在学习新类的过程中辅助模型抵抗灾难性遗忘.

##### GEM

基于数据约束的类别增量学习算法利用旧类数据可以进行数据重放, 通过复习旧类知识抵抗灾难性遗忘. 那么, 是否还有其他思路利用旧类范例集? 基于**数据约束**的算法旨在利用旧类数据约束模型的更新过程, 并使用范例集指导模型的更新策略, 以抵抗灾难性遗忘. 其中最有代表性的工作是 GEM[46] , GEM 希望找到最优的模型𝑓 ∗ 满足:

![image-20231103091055051](C:\Users\hanabi\AppData\Roaming\Typora\typora-user-images\image-20231103091055051.png)

其中, $𝑓_{𝑏−1}$​ 代表上一阶段训练结束时的增量模型.公式3表明, GEM 的优化目标是找到能够在新任务上最小化分类损失的模型 𝑓, 同时要保证其在范例集上的分类损失不大于上一阶段模型在范例集上的损失. 由于范例集中的样本均来自旧类, 这一约束一定程度上维持了模型在旧类上的性能. 为了考察模型在样本上的损失, GEM 计算模型在范例集上的梯度 𝑔ℰ 和在当前任务样本集上的梯度 𝑔. 如果二者夹角大于 90°, 则将当前任务的梯度投影到距离其夹角最小的范例集的梯度方向上, 以满足上述约束. 这个问题被进一步转化为二次规划问题进行求解. GEM 的想法非常直观: 只要能够保证模型在范例集上的损失不增大, 那么就认为模型在旧类上的性能就不会下降。





![image-20231106184739264](C:\Users\hanabi\AppData\Roaming\Typora\typora-user-images\image-20231106184739264.png)



#### 参考资料 

https://www.lamda.nju.edu.cn/zhoudw/file/cil_survey.pdf