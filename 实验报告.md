## 实验报告

201220138 白志欣

框架v1(20231115)



#### GEM-introduction

##### 范例集

按照类别增量学习过程的定义, 模型在学习第 𝑏 个任务时仅能获取当前的训练集$D^b$. 仅使用$D^b$更新模型很容易遭受灾难性遗忘. 因此, 当前类别增量学习的主流算法提出保存<u>一个额外的范例集合</u> (*exemplar/example set*), 记作$\epsilon$, 用于为每个见过的类别保存一定数目的代表性样本. 保存的样本可以在学习新类的过程中辅助模型抵抗灾难性遗忘.

##### GEM

基于数据约束的类别增量学习算法利用旧类数据可以进行数据重放, 通过复习旧类知识抵抗灾难性遗忘. 那么, 是否还有其他思路利用旧类范例集? 基于**数据约束**的算法旨在利用旧类数据约束模型的更新过程, 并使用范例集指导模型的更新策略, 以抵抗灾难性遗忘. 其中最有代表性的工作是 GEM[46] , GEM 希望找到最优的模型𝑓 ∗ 满足:

![image-20231103091055051](./2.png)

其中, $𝑓_{𝑏−1}$​ 代表上一阶段训练结束时的增量模型.公式3表明, GEM 的优化目标是找到能够在新任务上最小化分类损失的模型 𝑓, 同时要保证其在范例集上的分类损失不大于上一阶段模型在范例集上的损失. 由于范例集中的样本均来自旧类, 这一约束一定程度上维持了模型在旧类上的性能. 为了考察模型在样本上的损失, **GEM 计算模型在范例集上的梯度 𝑔ℰ 和在当前任务样本集上的梯度 𝑔. 如果二者夹角大于 90°, 则将当前任务的梯度投影到距离其夹角最小的范例集的梯度方向上, 以满足上述约束**. 这个问题被进一步转化为二次规划问题进行求解. GEM 的想法非常直观: 只要能够保证模型在范例集上的损失不增大, 那么就认为模型在旧类上的性能就不会下降。

![image-20231106184739264](./1.png)

#### 算法流程



#### 核心函数

observe()

待整理，书写注释

```python
def observe(self, data):
        # get data and labels
        x, y = data['image'], data['label']
        x = x.to(self.device)
        y = y.to(self.device)
        
        # update memory
        if self.t != self.old_task:
            self.observed_tasks.append(self.t)
            self.old_task = self.t

        # Update ring buffer storing examples from current task
        #bsz = y.data.size(0)
        bsz = y.size(0)
        endcnt = min(self.mem_cnt + bsz, self.n_memories)
        effbsz = endcnt - self.mem_cnt
        '''
        self.memory_data[self.t, self.mem_cnt: endcnt].copy_(
            x.data[: effbsz])
        '''
        #print(self.memory_data[self.t, self.mem_cnt: endcnt].size())
        #print(x[: effbsz].size())
        self.memory_data[self.t, self.mem_cnt: endcnt].copy_(
            x[: effbsz])
        if bsz == 1:
            self.memory_labs[self.t, self.mem_cnt] = y.data[0]
        else:
            '''
            self.memory_labs[self.t, self.mem_cnt: endcnt].copy_(
                y.data[: effbsz])
            '''
            self.memory_labs[self.t, self.mem_cnt: endcnt].copy_(
                y[: effbsz])
        self.mem_cnt += effbsz
        if self.mem_cnt == self.n_memories:
            self.mem_cnt = 0

        
        # compute gradient on previous tasks
        if len(self.observed_tasks) > 1:
            for tt in range(len(self.observed_tasks) - 1):
                self.zero_grad()
                # fwd/bwd on the examples in the memory
                past_task = self.observed_tasks[tt]

                #offset1, offset2 = compute_offsets(past_task, self.nc_per_task)
                offset1, offset2 = compute_offsets(self.t, self.nc_per_task)
                
                
                '''
                print(self.forward(
                        self.memory_data[past_task],
                        past_task)[:, offset1: offset2].device)
                
                print((self.memory_labs[past_task] - offset1).device)
                '''
                '''
                print(self.forward(
                        self.memory_data[past_task],
                        past_task)[:, offset1: offset2])
                print(self.memory_labs[past_task] - offset1)
                
                
                _, predicted = torch.max(self.forward(
                        self.memory_data[past_task],
                        past_task)[:, offset1: offset2], 1)  
                
                
                print(predicted)
                print(self.memory_labs[past_task] - offset1)
                '''
                #print(self.memory_data[past_task])
                #print(self.memory_labs[past_task] - offset1)
                '''
                ptloss = self.ce(
                    self.forward(
                        self.memory_data[past_task],
                        past_task)[:, offset1: offset2],
                    (self.memory_labs[past_task] - offset1).to(self.device))
                '''
                '''
                ptloss = self.ce(
                    self.forward(
                        self.memory_data[past_task],
                        self.t)[:, offset1: offset2],
                    (self.memory_labs[past_task] - offset1).to(self.device))
                '''
                ptloss = self.ce(
                    self.forward(
                        self.memory_data[past_task],
                        self.t),
                    (self.memory_labs[past_task] - offset1).to(self.device))
                ptloss.backward()
                store_grad(self.parameters, self.grads, self.grad_dims,
                           past_task)
        

        # now compute the grad on the current minibatch
        self.zero_grad()

        offset1, offset2 = compute_offsets(self.t, self.nc_per_task)
        #output = self.forward(x, self.t)[:, offset1: offset2]
        output = self.forward(x, self.t)
        _, predicted = torch.max(output, 1)  
        #print(predicted)
        #print(y)
        correct = (predicted == (y - offset1)).sum().item()  
        total = y.size(0) 
        acc = correct / total 

        loss = self.ce(output, y - offset1)
        loss.backward()

        # check if gradient violates constraints
        # 不满足约束会更新梯度
        #to-doS
        if len(self.observed_tasks) > 1:
            # copy gradient
            store_grad(self.parameters, self.grads, self.grad_dims, self.t)
            #indx = torch.cuda.LongTensor(self.observed_tasks[:-1]) if self.gpu \
            #    else torch.LongTensor(self.observed_tasks[:-1])
            indx = torch.LongTensor(self.observed_tasks[:-1])
            indx = indx.to(self.device)
            self.grads = self.grads.to(self.device)
            result = self.grads.index_select(1, indx)
            dotp = torch.mm((self.grads[:, self.t].unsqueeze(0)).to(self.device),
                            result)
            dotp.to(self.device)
            if (dotp < 0).sum() != 0:
                #project2cone2(self.grads[:, self.t].unsqueeze(1),
                              #self.grads.index_select(1, indx), self.margin)
                project2cone2(self.grads[:, self.t].unsqueeze(1),
                              self.grads.index_select(1, indx))
                # copy gradients back
                overwrite_grad(self.parameters, self.grads[:, self.t],
                               self.grad_dims)
        #self.opt.step()

        return output, acc, loss
```

inference

#### 框架修改







#### 实验设置

设计参照https://www.lamda.nju.edu.cn/zhoudw/file/cil_survey.pdf

实验基于pytorch实现，在NVIDIA 3090上运行（由于框架未设置并行训练，所以使用单卡），使用SGD优化器训练 170 轮, 初始学习率为 0.1, 并在第 80和120 轮衰减为 0.1 倍. 优化器的动量 (momentum)参数设定为 0.9, 权重衰减系数 (weightdecay) 设定为 2e-4, 模型训练阶段的 batchsize 设定为 128. 优化器学习率在每个新的的增量任务到来时重置为 0.1.对于 CIFAR100, 使用 ResNet32作为主干网络(backbone)



设置样例：`code/config/gem.yaml`

```yaml
includes:
  - headers/data.yaml
  - headers/device.yaml
  - headers/model.yaml
  - headers/optimizer.yaml
  - backbones/resnet12.yaml

data_root: /data/bzx_yjy/cifar100
image_size: 32
  
save_path: ./

# data
init_cls_num: 20
inc_cls_num: 20
task_num: 5


epoch: 170
device_ids: 0
n_gpu: 1
val_per_epoch: 5


batch_size: 128


optimizer:
  name: SGD
  kwargs:
    lr: 0.1
    momentum: 0.9
    weight_decay: 2e-4



lr_scheduler:
  name: MultiStepLR
  kwargs:
    gamma: 0.1
    milestones: [80, 120]

backbone:
  name: resnet32
  kwargs:

#useless
buffer:
  name: LinearBuffer
  kwargs:
    buffer_size: 0
    batch_size: 32
    strategy: random     # random, equal_random, reservoir, herding

classifier:
  name: GEM
  kwargs:
    num_class: 100
    feat_dim: 64
    #feat_dim: 512
    n_memories: 2000
    n_task: 5
    memory_strength: 0
```



按照基准设定, 存储 2,000 个样本作为范例集样本, 即为每个类存储 20 个范例集样本. 这些样本是基于群聚[80] 方式采样, 选择存储距离类别中心最近的样本. （待修改）



#### 实验结果对比



#### 参考资料 

https://www.lamda.nju.edu.cn/zhoudw/file/cil_survey.pdf