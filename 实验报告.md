## å®éªŒæŠ¥å‘Š

201220138 ç™½å¿—æ¬£

æ¡†æ¶v1(20231115)



#### GEM-introduction

##### èŒƒä¾‹é›†

æŒ‰ç…§ç±»åˆ«å¢é‡å­¦ä¹ è¿‡ç¨‹çš„å®šä¹‰, æ¨¡å‹åœ¨å­¦ä¹ ç¬¬ ğ‘ ä¸ªä»»åŠ¡æ—¶ä»…èƒ½è·å–å½“å‰çš„è®­ç»ƒé›†$D^b$. ä»…ä½¿ç”¨$D^b$æ›´æ–°æ¨¡å‹å¾ˆå®¹æ˜“é­å—ç¾éš¾æ€§é—å¿˜. å› æ­¤, å½“å‰ç±»åˆ«å¢é‡å­¦ä¹ çš„ä¸»æµç®—æ³•æå‡ºä¿å­˜<u>ä¸€ä¸ªé¢å¤–çš„èŒƒä¾‹é›†åˆ</u> (*exemplar/example set*), è®°ä½œ$\epsilon$, ç”¨äºä¸ºæ¯ä¸ªè§è¿‡çš„ç±»åˆ«ä¿å­˜ä¸€å®šæ•°ç›®çš„ä»£è¡¨æ€§æ ·æœ¬. ä¿å­˜çš„æ ·æœ¬å¯ä»¥åœ¨å­¦ä¹ æ–°ç±»çš„è¿‡ç¨‹ä¸­è¾…åŠ©æ¨¡å‹æŠµæŠ—ç¾éš¾æ€§é—å¿˜.

##### GEM

åŸºäºæ•°æ®çº¦æŸçš„ç±»åˆ«å¢é‡å­¦ä¹ ç®—æ³•åˆ©ç”¨æ—§ç±»æ•°æ®å¯ä»¥è¿›è¡Œæ•°æ®é‡æ”¾, é€šè¿‡å¤ä¹ æ—§ç±»çŸ¥è¯†æŠµæŠ—ç¾éš¾æ€§é—å¿˜. é‚£ä¹ˆ, æ˜¯å¦è¿˜æœ‰å…¶ä»–æ€è·¯åˆ©ç”¨æ—§ç±»èŒƒä¾‹é›†? åŸºäº**æ•°æ®çº¦æŸ**çš„ç®—æ³•æ—¨åœ¨åˆ©ç”¨æ—§ç±»æ•°æ®çº¦æŸæ¨¡å‹çš„æ›´æ–°è¿‡ç¨‹, å¹¶ä½¿ç”¨èŒƒä¾‹é›†æŒ‡å¯¼æ¨¡å‹çš„æ›´æ–°ç­–ç•¥, ä»¥æŠµæŠ—ç¾éš¾æ€§é—å¿˜. å…¶ä¸­æœ€æœ‰ä»£è¡¨æ€§çš„å·¥ä½œæ˜¯ GEM[46] , GEM å¸Œæœ›æ‰¾åˆ°æœ€ä¼˜çš„æ¨¡å‹ğ‘“ âˆ— æ»¡è¶³:

![image-20231103091055051](./2.png)

å…¶ä¸­, $ğ‘“_{ğ‘âˆ’1}$â€‹ ä»£è¡¨ä¸Šä¸€é˜¶æ®µè®­ç»ƒç»“æŸæ—¶çš„å¢é‡æ¨¡å‹.å…¬å¼3è¡¨æ˜, GEM çš„ä¼˜åŒ–ç›®æ ‡æ˜¯æ‰¾åˆ°èƒ½å¤Ÿåœ¨æ–°ä»»åŠ¡ä¸Šæœ€å°åŒ–åˆ†ç±»æŸå¤±çš„æ¨¡å‹ ğ‘“, åŒæ—¶è¦ä¿è¯å…¶åœ¨èŒƒä¾‹é›†ä¸Šçš„åˆ†ç±»æŸå¤±ä¸å¤§äºä¸Šä¸€é˜¶æ®µæ¨¡å‹åœ¨èŒƒä¾‹é›†ä¸Šçš„æŸå¤±. ç”±äºèŒƒä¾‹é›†ä¸­çš„æ ·æœ¬å‡æ¥è‡ªæ—§ç±», è¿™ä¸€çº¦æŸä¸€å®šç¨‹åº¦ä¸Šç»´æŒäº†æ¨¡å‹åœ¨æ—§ç±»ä¸Šçš„æ€§èƒ½. ä¸ºäº†è€ƒå¯Ÿæ¨¡å‹åœ¨æ ·æœ¬ä¸Šçš„æŸå¤±, **GEM è®¡ç®—æ¨¡å‹åœ¨èŒƒä¾‹é›†ä¸Šçš„æ¢¯åº¦ ğ‘”â„° å’Œåœ¨å½“å‰ä»»åŠ¡æ ·æœ¬é›†ä¸Šçš„æ¢¯åº¦ ğ‘”. å¦‚æœäºŒè€…å¤¹è§’å¤§äº 90Â°, åˆ™å°†å½“å‰ä»»åŠ¡çš„æ¢¯åº¦æŠ•å½±åˆ°è·ç¦»å…¶å¤¹è§’æœ€å°çš„èŒƒä¾‹é›†çš„æ¢¯åº¦æ–¹å‘ä¸Š, ä»¥æ»¡è¶³ä¸Šè¿°çº¦æŸ**. è¿™ä¸ªé—®é¢˜è¢«è¿›ä¸€æ­¥è½¬åŒ–ä¸ºäºŒæ¬¡è§„åˆ’é—®é¢˜è¿›è¡Œæ±‚è§£. GEM çš„æƒ³æ³•éå¸¸ç›´è§‚: åªè¦èƒ½å¤Ÿä¿è¯æ¨¡å‹åœ¨èŒƒä¾‹é›†ä¸Šçš„æŸå¤±ä¸å¢å¤§, é‚£ä¹ˆå°±è®¤ä¸ºæ¨¡å‹åœ¨æ—§ç±»ä¸Šçš„æ€§èƒ½å°±ä¸ä¼šä¸‹é™ã€‚

![image-20231106184739264](./1.png)

#### ç®—æ³•æµç¨‹



#### æ ¸å¿ƒå‡½æ•°

observe()

å¾…æ•´ç†ï¼Œä¹¦å†™æ³¨é‡Š

```python
def observe(self, data):
        # get data and labels
        x, y = data['image'], data['label']
        x = x.to(self.device)
        y = y.to(self.device)
        
        # update memory
        if self.t != self.old_task:
            self.observed_tasks.append(self.t)
            self.old_task = self.t

        # Update ring buffer storing examples from current task
        #bsz = y.data.size(0)
        bsz = y.size(0)
        endcnt = min(self.mem_cnt + bsz, self.n_memories)
        effbsz = endcnt - self.mem_cnt
        '''
        self.memory_data[self.t, self.mem_cnt: endcnt].copy_(
            x.data[: effbsz])
        '''
        #print(self.memory_data[self.t, self.mem_cnt: endcnt].size())
        #print(x[: effbsz].size())
        self.memory_data[self.t, self.mem_cnt: endcnt].copy_(
            x[: effbsz])
        if bsz == 1:
            self.memory_labs[self.t, self.mem_cnt] = y.data[0]
        else:
            '''
            self.memory_labs[self.t, self.mem_cnt: endcnt].copy_(
                y.data[: effbsz])
            '''
            self.memory_labs[self.t, self.mem_cnt: endcnt].copy_(
                y[: effbsz])
        self.mem_cnt += effbsz
        if self.mem_cnt == self.n_memories:
            self.mem_cnt = 0

        
        # compute gradient on previous tasks
        if len(self.observed_tasks) > 1:
            for tt in range(len(self.observed_tasks) - 1):
                self.zero_grad()
                # fwd/bwd on the examples in the memory
                past_task = self.observed_tasks[tt]

                #offset1, offset2 = compute_offsets(past_task, self.nc_per_task)
                offset1, offset2 = compute_offsets(self.t, self.nc_per_task)
                
                
                '''
                print(self.forward(
                        self.memory_data[past_task],
                        past_task)[:, offset1: offset2].device)
                
                print((self.memory_labs[past_task] - offset1).device)
                '''
                '''
                print(self.forward(
                        self.memory_data[past_task],
                        past_task)[:, offset1: offset2])
                print(self.memory_labs[past_task] - offset1)
                
                
                _, predicted = torch.max(self.forward(
                        self.memory_data[past_task],
                        past_task)[:, offset1: offset2], 1)  
                
                
                print(predicted)
                print(self.memory_labs[past_task] - offset1)
                '''
                #print(self.memory_data[past_task])
                #print(self.memory_labs[past_task] - offset1)
                '''
                ptloss = self.ce(
                    self.forward(
                        self.memory_data[past_task],
                        past_task)[:, offset1: offset2],
                    (self.memory_labs[past_task] - offset1).to(self.device))
                '''
                '''
                ptloss = self.ce(
                    self.forward(
                        self.memory_data[past_task],
                        self.t)[:, offset1: offset2],
                    (self.memory_labs[past_task] - offset1).to(self.device))
                '''
                ptloss = self.ce(
                    self.forward(
                        self.memory_data[past_task],
                        self.t),
                    (self.memory_labs[past_task] - offset1).to(self.device))
                ptloss.backward()
                store_grad(self.parameters, self.grads, self.grad_dims,
                           past_task)
        

        # now compute the grad on the current minibatch
        self.zero_grad()

        offset1, offset2 = compute_offsets(self.t, self.nc_per_task)
        #output = self.forward(x, self.t)[:, offset1: offset2]
        output = self.forward(x, self.t)
        _, predicted = torch.max(output, 1)  
        #print(predicted)
        #print(y)
        correct = (predicted == (y - offset1)).sum().item()  
        total = y.size(0) 
        acc = correct / total 

        loss = self.ce(output, y - offset1)
        loss.backward()

        # check if gradient violates constraints
        # ä¸æ»¡è¶³çº¦æŸä¼šæ›´æ–°æ¢¯åº¦
        #to-doS
        if len(self.observed_tasks) > 1:
            # copy gradient
            store_grad(self.parameters, self.grads, self.grad_dims, self.t)
            #indx = torch.cuda.LongTensor(self.observed_tasks[:-1]) if self.gpu \
            #    else torch.LongTensor(self.observed_tasks[:-1])
            indx = torch.LongTensor(self.observed_tasks[:-1])
            indx = indx.to(self.device)
            self.grads = self.grads.to(self.device)
            result = self.grads.index_select(1, indx)
            dotp = torch.mm((self.grads[:, self.t].unsqueeze(0)).to(self.device),
                            result)
            dotp.to(self.device)
            if (dotp < 0).sum() != 0:
                #project2cone2(self.grads[:, self.t].unsqueeze(1),
                              #self.grads.index_select(1, indx), self.margin)
                project2cone2(self.grads[:, self.t].unsqueeze(1),
                              self.grads.index_select(1, indx))
                # copy gradients back
                overwrite_grad(self.parameters, self.grads[:, self.t],
                               self.grad_dims)
        #self.opt.step()

        return output, acc, loss
```

inference

#### æ¡†æ¶ä¿®æ”¹







#### å®éªŒè®¾ç½®

è®¾è®¡å‚ç…§https://www.lamda.nju.edu.cn/zhoudw/file/cil_survey.pdf

å®éªŒåŸºäºpytorchå®ç°ï¼Œåœ¨NVIDIA 3090ä¸Šè¿è¡Œï¼ˆç”±äºæ¡†æ¶æœªè®¾ç½®å¹¶è¡Œè®­ç»ƒï¼Œæ‰€ä»¥ä½¿ç”¨å•å¡ï¼‰ï¼Œä½¿ç”¨SGDä¼˜åŒ–å™¨è®­ç»ƒ 170 è½®, åˆå§‹å­¦ä¹ ç‡ä¸º 0.1, å¹¶åœ¨ç¬¬ 80å’Œ120 è½®è¡°å‡ä¸º 0.1 å€. ä¼˜åŒ–å™¨çš„åŠ¨é‡ (momentum)å‚æ•°è®¾å®šä¸º 0.9, æƒé‡è¡°å‡ç³»æ•° (weightdecay) è®¾å®šä¸º 2e-4, æ¨¡å‹è®­ç»ƒé˜¶æ®µçš„ batchsize è®¾å®šä¸º 128. ä¼˜åŒ–å™¨å­¦ä¹ ç‡åœ¨æ¯ä¸ªæ–°çš„çš„å¢é‡ä»»åŠ¡åˆ°æ¥æ—¶é‡ç½®ä¸º 0.1.å¯¹äº CIFAR100, ä½¿ç”¨ ResNet32ä½œä¸ºä¸»å¹²ç½‘ç»œ(backbone)



è®¾ç½®æ ·ä¾‹ï¼š`code/config/gem.yaml`

```yaml
includes:
  - headers/data.yaml
  - headers/device.yaml
  - headers/model.yaml
  - headers/optimizer.yaml
  - backbones/resnet12.yaml

data_root: /data/bzx_yjy/cifar100
image_size: 32
  
save_path: ./

# data
init_cls_num: 20
inc_cls_num: 20
task_num: 5


epoch: 170
device_ids: 0
n_gpu: 1
val_per_epoch: 5


batch_size: 128


optimizer:
  name: SGD
  kwargs:
    lr: 0.1
    momentum: 0.9
    weight_decay: 2e-4



lr_scheduler:
  name: MultiStepLR
  kwargs:
    gamma: 0.1
    milestones: [80, 120]

backbone:
  name: resnet32
  kwargs:

#useless
buffer:
  name: LinearBuffer
  kwargs:
    buffer_size: 0
    batch_size: 32
    strategy: random     # random, equal_random, reservoir, herding

classifier:
  name: GEM
  kwargs:
    num_class: 100
    feat_dim: 64
    #feat_dim: 512
    n_memories: 2000
    n_task: 5
    memory_strength: 0
```



æŒ‰ç…§åŸºå‡†è®¾å®š, å­˜å‚¨ 2,000 ä¸ªæ ·æœ¬ä½œä¸ºèŒƒä¾‹é›†æ ·æœ¬, å³ä¸ºæ¯ä¸ªç±»å­˜å‚¨ 20 ä¸ªèŒƒä¾‹é›†æ ·æœ¬. è¿™äº›æ ·æœ¬æ˜¯åŸºäºç¾¤èš[80] æ–¹å¼é‡‡æ ·, é€‰æ‹©å­˜å‚¨è·ç¦»ç±»åˆ«ä¸­å¿ƒæœ€è¿‘çš„æ ·æœ¬. ï¼ˆå¾…ä¿®æ”¹ï¼‰



#### å®éªŒç»“æœå¯¹æ¯”



#### å‚è€ƒèµ„æ–™ 

https://www.lamda.nju.edu.cn/zhoudw/file/cil_survey.pdf